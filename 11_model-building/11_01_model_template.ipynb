{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Template\n",
    "\n",
    "--Please replace this Markdown cell with a description of your trading model.--\n",
    "\n",
    "This is a template of a notebook we can use to build a trading model. Some code cells can be (should be) copied over to the trading algorithm with which you may perform backtesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Settings\n",
    "\n",
    "In the code cell below, switch the production/development commented sections as necessary, and then copy the code into the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy to the algorithm\n",
    "\n",
    "# Module Imports\n",
    "# --------------------\n",
    "import quantopian.optimize as opt\n",
    "from quantopian.pipeline import Pipeline\n",
    "from quantopian.pipeline.factors import CustomFactor\n",
    "from quantopian.pipeline.data.builtin import USEquityPricing\n",
    "from quantopian.pipeline.data.morningstar import Fundamentals\n",
    "\n",
    "from quantopian.pipeline.filters import QTradableStocksUS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment Settings\n",
    "# --------------------\n",
    "## Production \n",
    "# universe = QTradableStocksUS()\n",
    "# mask = {'mask': universe}\n",
    "\n",
    "## Development\n",
    "universe = QTradableStocksUS()\n",
    "mask = {'mask': universe}\n",
    "\n",
    "\n",
    "# Global Configuration\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not copy this code cell into the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.research import run_pipeline\n",
    "\n",
    "# Leave at least 6 months holding period.\n",
    "\n",
    "## Production \n",
    "# start_date = datetime.strptime('01/01/2018', '%m/%d/%Y')\n",
    "# end_date = datetime.strptime('06/01/2019', '%m/%d/%Y')\n",
    "\n",
    "## Development\n",
    "start_date = datetime.strptime('03/01/2020', '%m/%d/%Y')\n",
    "end_date = datetime.strptime('03/02/2020', '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helpers\n",
    "\n",
    "The code below can be copied directly into the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy to the algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertion code, if needed. Use to test out helper functions.\n",
    "# Here is an example of normalize() function's assertion.\n",
    "\n",
    "def assert_normalize():\n",
    "    a = normalize(np.array([-300, -200, -100, 0, 100, 200, 300]))\n",
    "    b = normalize(np.array([-200, -100, 0, 100, 200, 300, 400]))\n",
    "    c = normalize(np.array([-3, -2, -1, 0, 1, 2, 3]))\n",
    "    d = normalize(np.array([1,1,1,1,1]))\n",
    "    assert type(a) == pd.core.series.Series, \"Return type must be a pandas.core.series.Series object\"\n",
    "    assert (a == b).all(), \"Demean should adjust the mean properly\"\n",
    "    assert (a == c).all(), \"Incorrect normalization result\"\n",
    "    assert (d.sum() == 0), \"If an array has the same value for all elements, normalized value should be zero for all\"\n",
    "    return True\n",
    "assert_normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Pipeline\n",
    "\n",
    "The code below can be copied directly into the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy to the algorithm\n",
    "\n",
    "def make_alpha_factors():\n",
    "    factors = {}\n",
    "    # Todo: Create factors here. One of the factors must be named `a_combined`.\n",
    "    factors['a_combined'] = ...\n",
    "    \n",
    "    return factors\n",
    "                                        \n",
    "\n",
    "def make_pipeline():\n",
    "    alpha_factors = make_alpha_factors()\n",
    "    factors = {a: alpha_factors[a] for a in alpha_factors}\n",
    "    pipe = Pipeline(columns=factors, screen=universe)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline()\n",
    "mdf = run_pipeline(pipe, start_date, end_date).dropna(how='all')\n",
    "mdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Pipeline Result\n",
    "\n",
    "### 5.1. Validating the normalization process\n",
    "\n",
    "Checking combined alpha of the first date. Should be close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_available_date = mdf.index.get_level_values(0)[0]\n",
    "selector = mdf.index.get_level_values(0) == first_available_date\n",
    "mdf.loc[selector]['a_combined'].abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Distribution of values\n",
    "\n",
    "More spread, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Correlation between factors\n",
    "\n",
    "Correlation close to 1.0 (or -1.0) means the factor is probably not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.fillna(0).corr()  # Filling NaNs with 0 assumes empty values are the mean (z-score/rank of 0)\n",
    "# mdf.corr()  # Drops NaNs; results are not much different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Number of stocks to trade\n",
    "\n",
    "1. Total number of stocks considered for trading throughout all dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.index.get_level_values(1).unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many stocks on average do we trade each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.dropna().groupby(level=0).agg('count').mean()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Alpha Factors\n",
    " \n",
    "We have a function below to extract alpha factors from the `factors` DataFrame we have created above. It does two things by default:\n",
    "\n",
    "1. Replace `np.inf` and `np.nan` to 0, and\n",
    "2. get the `factors['a_combined']` Series.\n",
    "\n",
    "Although you may technically combine the alphas in this function, it is preferable to do the alpha combination step on the \"Build Pipeline\" step above. The reasoning:\n",
    "\n",
    "The algorithm environment requires this function to accept a $1 \\times s$ DataFrame where $s$ is the number of assets, while this research environment requires a $d \\times s$ DataFrame where $d$ is the number of dates. Due to this, performing complex operations (such as getting only the extreme values of the combined alpha) may take a very long time on the Notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Adjust as necessary. In here, columns that begins with 'a_' would be\n",
    "#       considered alpha factors. The rest are for analysis purposes.\n",
    "\n",
    "filter_col = [col for col in mdf if col.startswith('a_')]\n",
    "alphas = mdf.loc[:, filter_col]\n",
    "\n",
    "# Note: Copy to the algorithm\n",
    "\n",
    "def get_alpha(factors):\n",
    "    # Replace infs and NaNs\n",
    "    factors[np.isinf(factors)] = np.nan\n",
    "    factors.fillna(0, inplace=True)\n",
    "\n",
    "    combined_alpha = factors['a_combined']\n",
    "    return combined_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trading algorithm will use the `combined_alpha` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_alpha = get_alpha(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Alphas\n",
    "\n",
    "### 7.1. Alphas' Statistics\n",
    "\n",
    "Present some statistics of the alphas here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many % of our signals are shorts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_daily_signals = alphas.groupby(level=0).count().median()\n",
    "percent_short = float(combined_alpha[combined_alpha < 0.0].count()) \\\n",
    "                / float(combined_alpha.count())*100\n",
    "\n",
    "print(\"Median number of daily signals: {}\".format(median_daily_signals))\n",
    "print(\"% of short signals: {}%\".format(percent_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Alphalens Analysis\n",
    "\n",
    "`ANALYZE_ALL` settings:\n",
    "\n",
    "1. `True`: Analyze all alpha factors in variable `alphas` and the final `combined_alpha` variable.\n",
    "2. `False`: Analyze only the final factor i.e. the `combined_alpha` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZE_ALL = True\n",
    "\n",
    "if ANALYZE_ALL:\n",
    "    alphas_view = alphas.copy()\n",
    "else:\n",
    "    alphas_view = pd.DataFrame({'aa_combined': combined_alpha})\n",
    "alphas_view.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# 1 day, 1 week, 1 month, 1 quarter\n",
    "periods = [1, 5, 22, 64]\n",
    "\n",
    "# Get pricing data (extends 6 months to minimize dropping in Alphalens)\n",
    "new_start_date = start_date - relativedelta(months=6)\n",
    "new_end_date = end_date + relativedelta(months=6)\n",
    "assets = alphas_view.reset_index()['level_1'].unique()\n",
    "dates = alphas_view.reset_index()['level_0'].unique()\n",
    "prices = get_pricing(assets, start_date=new_start_date, end_date=new_end_date, fields='close_price')\n",
    "prices.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al\n",
    "from scipy import stats\n",
    "\n",
    "def get_table(ic_data, ab_data):\n",
    "    summary_table = pd.DataFrame()\n",
    "    summary_table[\"Ann. Alpha\"] = ab_data.loc['Ann. alpha']\n",
    "    summary_table[\"beta\"] = ab_data.loc['beta']\n",
    "    summary_table[\"IC Mean\"] = ic_data.mean()\n",
    "    summary_table[\"IC Std.\"] = ic_data.std()\n",
    "    summary_table[\"Risk-Adjusted IC\"] = \\\n",
    "        ic_data.mean() / ic_data.std()\n",
    "    t_stat, p_value = stats.ttest_1samp(ic_data, 0)\n",
    "    summary_table[\"p-value(IC)\"] = p_value\n",
    "\n",
    "    return summary_table.apply(lambda x: x.round(3)).T\n",
    "\n",
    "\n",
    "results = None\n",
    "for i, col in enumerate(sorted(alphas_view.columns)):\n",
    "    if i > 0:\n",
    "        print('')\n",
    "    print(col)\n",
    "    \n",
    "    # Get the factor data\n",
    "    data = alphas_view[col]\n",
    "    data = data[data != 0].dropna()\n",
    "#     try:\n",
    "    factor_data = al.utils.get_clean_factor_and_forward_returns(data,\n",
    "                                                                prices,\n",
    "                                                                quantiles=5,\n",
    "                                                                periods=periods,\n",
    "                                                                max_loss=1.\n",
    "                                                               )\n",
    "\n",
    "    # Output the results\n",
    "    ic = al.performance.factor_information_coefficient(factor_data)\n",
    "    ic.columns = pd.MultiIndex.from_product([[col], ic.columns])\n",
    "\n",
    "    returns = al.performance.factor_returns(factor_data)\n",
    "    ab = al.performance.factor_alpha_beta(factor_data, returns=returns)\n",
    "    ab.columns = pd.MultiIndex.from_product([[col], ab.columns])\n",
    "\n",
    "    table = get_table(ic, ab)\n",
    "\n",
    "    if results is None:\n",
    "        results = table\n",
    "    else:\n",
    "        results = pd.concat([results, table], axis=1)\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print('Error: {}'.format(e))\n",
    "#         continue\n",
    "        \n",
    "temp = None\n",
    "i = 0\n",
    "unique_vals = results.columns.get_level_values(0).unique()\n",
    "for j, factor in enumerate(sorted(unique_vals)):\n",
    "    i += 1\n",
    "    res = results.xs(factor, axis=1, level=0, drop_level=False)\n",
    "    \n",
    "    if temp is None:\n",
    "        temp = res\n",
    "    else:\n",
    "        temp = pd.concat([temp, res], axis=1)\n",
    "        \n",
    "    if i > 4 or j == len(unique_vals) - 1:\n",
    "        display(temp)\n",
    "        temp = None\n",
    "        i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tear Sheet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf\n",
    "\n",
    "from quantopian.research.experimental import get_factor_returns, get_factor_loadings\n",
    "\n",
    "MORNINGSTAR_SECTOR_CODES = {\n",
    "     -1: 'Misc',\n",
    "    101: 'Basic Materials',\n",
    "    102: 'Consumer Cyclical',\n",
    "    103: 'Financial Services',\n",
    "    104: 'Real Estate',\n",
    "    205: 'Consumer Defensive',\n",
    "    206: 'Healthcare',\n",
    "    207: 'Utilities',\n",
    "    308: 'Communication Services',\n",
    "    309: 'Energy',\n",
    "    310: 'Industrials',\n",
    "    311: 'Technology' ,    \n",
    "}\n",
    "\n",
    "# Load risk factor loadings and returns\n",
    "factor_loadings = get_factor_loadings(assets, start_date, new_end_date)\n",
    "factor_returns = get_factor_returns(start_date, new_end_date)\n",
    "\n",
    "# Fix a bug in the risk returns\n",
    "factor_returns.loc[factor_returns.value.idxmax(), 'value'] = 0\n",
    "\n",
    "def calc_perf_attrib(portfolio_returns, portfolio_pos, factor_returns, factor_loadings):\n",
    "    start = portfolio_returns.index[0]\n",
    "    end = portfolio_returns.index[-1]\n",
    "    factor_loadings.index = factor_loadings.index.set_names(['dt', 'ticker'])\n",
    "    portfolio_pos.index = portfolio_pos.index.set_names(['dt'])\n",
    "    \n",
    "    portfolio_pos = portfolio_pos.drop('cash', axis=1)\n",
    "    portfolio_pos.columns.name = 'ticker'\n",
    "    portfolio_pos.columns = portfolio_pos.columns.astype('int')\n",
    "    \n",
    "    return ep.perf_attrib(\n",
    "        portfolio_returns, \n",
    "        portfolio_pos.stack().dropna(),\n",
    "        factor_returns.loc[start:end], \n",
    "        factor_loadings.loc[start:end])\n",
    "\n",
    "def plot_exposures(risk_exposures, ax=None):\n",
    "    rep = risk_exposures.stack().reset_index()\n",
    "    rep.columns = ['dt', 'factor', 'exposure']\n",
    "    sns.boxplot(x='exposure', y='factor', data=rep, orient='h', ax=ax, order=risk_exposures.columns[::-1])\n",
    "\n",
    "def compute_turnover(df):\n",
    "    return df.dropna().unstack().dropna(how='all').fillna(0).diff().abs().sum(1)\n",
    "\n",
    "def get_max_median_position_concentration(expos):\n",
    "    longs = expos.loc[expos > 0]\n",
    "    shorts = expos.loc[expos < 0]\n",
    "\n",
    "    return expos.groupby(level=0).quantile([.05, .25, .5, .75, .95]).unstack()\n",
    "\n",
    "def compute_factor_stats(factor, pricing, factor_returns,\n",
    "                         factor_loadings, periods=range(1, 15),\n",
    "                         view=None):\n",
    "    factor_data_total = al.utils.get_clean_factor_and_forward_returns(\n",
    "        factor, \n",
    "        pricing,\n",
    "        quantiles=None,\n",
    "        bins=(-np.inf, 0, np.inf),\n",
    "        periods=periods,\n",
    "        cumulative_returns=False\n",
    "    )\n",
    "\n",
    "    portfolio_returns_total = al.performance.factor_returns(factor_data_total)\n",
    "    portfolio_returns_total.columns = portfolio_returns_total.columns.map(lambda x: int(x[:-1]))\n",
    "    for i in portfolio_returns_total.columns:\n",
    "        portfolio_returns_total[i] = portfolio_returns_total[i].shift(i)\n",
    "\n",
    "    portfolio_returns_specific = pd.DataFrame(columns=portfolio_returns_total.columns, index=portfolio_returns_total.index)\n",
    "    \n",
    "    # closure\n",
    "    def calc_perf_attrib_c(i, portfolio_returns_total=portfolio_returns_total, \n",
    "                           factor_data_total=factor_data_total, factor_returns=factor_returns, \n",
    "                           factor_loadings=factor_loadings):\n",
    "        return calc_perf_attrib(portfolio_returns_total[i], \n",
    "                                factor_data_total['factor'].unstack().assign(cash=0).shift(i), \n",
    "                                factor_returns, factor_loadings)\n",
    "    \n",
    "    if view is None:\n",
    "        perf_attrib = map(calc_perf_attrib_c, portfolio_returns_total.columns)\n",
    "    else:\n",
    "        perf_attrib = view.map_sync(calc_perf_attrib_c, portfolio_returns_total.columns)\n",
    "        \n",
    "    for i, pa in enumerate(perf_attrib):\n",
    "        if i == 0:\n",
    "            risk_exposures_portfolio = pa[0]\n",
    "            perf_attribution = pa[1]\n",
    "        portfolio_returns_specific[i + 1] = pa[1]['specific_returns']\n",
    "    \n",
    "    delay_sharpes_total = portfolio_returns_total.apply(ep.sharpe_ratio)\n",
    "    delay_sharpes_specific = portfolio_returns_specific.apply(ep.sharpe_ratio)\n",
    "    \n",
    "    turnover = compute_turnover(factor)\n",
    "    n_holdings = factor.groupby(level=0).count()\n",
    "    perc_holdings = get_max_median_position_concentration(factor)\n",
    "    \n",
    "    return {'factor_data_total': factor_data_total, \n",
    "            'portfolio_returns_total': portfolio_returns_total,\n",
    "            'portfolio_returns_specific': portfolio_returns_specific,\n",
    "            'risk_exposures_portfolio': risk_exposures_portfolio,\n",
    "            'perf_attribution': perf_attribution,\n",
    "            'delay_sharpes_total': delay_sharpes_total,\n",
    "            'delay_sharpes_specific': delay_sharpes_specific,\n",
    "            'turnover': turnover,\n",
    "            'n_holdings': n_holdings,\n",
    "            'perc_holdings': perc_holdings,\n",
    "    }\n",
    "\n",
    "def plot_overview_tear_sheet(factor, pricing, factor_returns, factor_loadings,\n",
    "                             periods=range(1, 15), view=None):\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    gs = plt.GridSpec(5, 4)\n",
    "    ax1 = plt.subplot(gs[0:2, 0:2])\n",
    "    \n",
    "    factor_stats = compute_factor_stats(factor, pricing, factor_returns, factor_loadings,\n",
    "                                        periods=periods, view=view)\n",
    "                         \n",
    "    sharpes = pd.DataFrame({'specific': factor_stats['delay_sharpes_specific'], \n",
    "                  'total': factor_stats['delay_sharpes_total']})\n",
    "#     display(sharpes)\n",
    "    sharpes.plot.bar(ax=ax1)\n",
    "    ax1.set(xlabel='delay', ylabel='IR')\n",
    "\n",
    "    ax2a = plt.subplot(gs[0, 2:4])\n",
    "    delay_cum_rets_total = factor_stats['portfolio_returns_total'][list(range(1, 5))].apply(ep.cum_returns)\n",
    "    delay_cum_rets_total.plot(ax=ax2a)\n",
    "    ax2a.set(title='Total returns', ylabel='Cumulative returns')\n",
    "    \n",
    "    ax2b = plt.subplot(gs[1, 2:4])\n",
    "    delay_cum_rets_specific = factor_stats['portfolio_returns_specific'][list(range(1, 5))].apply(ep.cum_returns)\n",
    "    delay_cum_rets_specific.plot(ax=ax2b)\n",
    "    ax2b.set(title='Specific returns', ylabel='Cumulative returns')\n",
    "    \n",
    "    ax3 = plt.subplot(gs[2:4, 0:2])\n",
    "    plot_exposures(factor_stats['risk_exposures_portfolio'].reindex(columns=factor_stats['perf_attribution'].columns), \n",
    "                   ax=ax3)\n",
    "\n",
    "    ax4 = plt.subplot(gs[2:4, 2])\n",
    "    ep.cum_returns_final(factor_stats['perf_attribution']).plot.barh(ax=ax4)\n",
    "    ax4.set(xlabel='Cumulative returns')\n",
    "\n",
    "    ax5 = plt.subplot(gs[2:4, 3], sharey=ax4)\n",
    "    factor_stats['perf_attribution'].apply(ep.annual_volatility).plot.barh(ax=ax5)\n",
    "    ax5.set(xlabel='Ann. volatility')\n",
    "\n",
    "    ax6 = plt.subplot(gs[-1, 0:2])\n",
    "    factor_stats['n_holdings'].plot(color='b', ax=ax6)\n",
    "    ax6.set_ylabel('# holdings', color='b')\n",
    "    ax6.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    ax62 = ax6.twinx()\n",
    "    factor_stats['turnover'].plot(color='r', ax=ax62)\n",
    "    ax62.set_ylabel('turnover', color='r')\n",
    "    ax62.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    ax7 = plt.subplot(gs[-1, 2:4])\n",
    "    factor_stats['perc_holdings'].plot(ax=ax7)\n",
    "    ax7.set_ylabel('Holdings Ratio')\n",
    "    \n",
    "    gs.tight_layout(fig)\n",
    "    \n",
    "    return fig, factor_stats, sharpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop through all columns\n",
    "results = None\n",
    "for i, col in enumerate(sorted(alphas_view.columns)):\n",
    "    if i > 0:\n",
    "        print('')\n",
    "    print(col)\n",
    "    \n",
    "    # Get the factor data\n",
    "    try:\n",
    "        data = alphas_view[col]\n",
    "        data = data[data != 0].dropna()\n",
    "        fig, factor_stats, sharpes = plot_overview_tear_sheet(data,\n",
    "                                                     prices,\n",
    "                                                     factor_returns,\n",
    "                                                     factor_loadings);\n",
    "        plt.show()\n",
    "        \n",
    "        sharpes.columns = pd.MultiIndex.from_product([[col], sharpes.columns])\n",
    "        if results is None:\n",
    "            results = sharpes\n",
    "        else:\n",
    "            results = pd.concat([results, sharpes], axis=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error: {}'.format(e))\n",
    "        continue\n",
    "        \n",
    "# results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
